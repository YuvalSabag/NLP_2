## **SkipGram with Negative Sampling (SGNS) for Word Embeddings**

This project implements the **SkipGram model with Negative Sampling (SGNS)** to learn word embeddings that capture semantic relationships between words. The SkipGram model is trained to predict context words for a given target word within a defined window, and through **negative sampling**, it efficiently learns high-quality embeddings that reflect co-occurrence patterns in text. These embeddings are applied to downstream tasks such as word similarity computation and analogy solving, offering hands-on experience with training word embeddings, exploring distributional semantics, and evaluating embedding quality using performance metrics and visualization. By training on various corpora, this project showcases the strengths of SGNS for semantic understanding in natural language processing.

Additionally, the project leverages multiple datasets and explores key NLP concepts such as **logistic regression**, **loss functions**, **gradient descent**, and **backpropagation**, providing insights into the strengths and limitations of distributional representations.
